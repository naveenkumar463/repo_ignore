{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://sjsu_cdw/config_files/\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import datetime\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "#Create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()\n",
    "CONFIG_PATH = os.environ.get('CONFIG_PATH')\n",
    "print(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://sjsu_cdw/raw_data/Admission/\n",
      "gs://sjsu_cdw/cdw_incremantal_load/TransformedData/Admissions/Dimension_Staging/\n"
     ]
    }
   ],
   "source": [
    "df_config = spark.read.option(\"header\", \"true\").csv(CONFIG_PATH+\"Config_ADM.csv\").collect()\n",
    "SOURCEFILEPATH = df_config[0]['SourceFile']\n",
    "TARGETFILEPATH = df_config[1]['adm_dim_path']\n",
    "fileName = \"CS_DS_ADAPPL_CNTR\"\n",
    "SEQUENCE_ID = df_config[0]['monotonically_inc_id']\n",
    "\n",
    "print(SOURCEFILEPATH)\n",
    "print(TARGETFILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all the required source tables\n",
    "sourceTablelist = [\"PS_ADM_APPLCTR_TBL\"]\n",
    "#Get the target table filename from the python filename\n",
    "targetFileName =  TARGETFILEPATH+fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "#Iterate through the source table list and load the data into a Spark dataframe. \n",
    "def createTempTables(sparkSess,sourceTablelist):\n",
    "    for tabList in sourceTablelist:\n",
    "        df_source = sparkSess.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").option(\"nullValue\",\" \").csv(SOURCEFILEPATH+tabList+\".csv\")\n",
    "        #df1 = df_source.na.fill('-')\n",
    "        #df2=df1.na.fill('1900-01-01 01:01:01 UTC')\n",
    "        #df2.printSchema()\n",
    "        #df_source.printSchema()\n",
    "        #Register the dataframe as a temporary table\n",
    "        df_source.registerTempTable(tabList)\n",
    "    return df_source\n",
    "df_source=createTempTables(spark,sourceTablelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a Spark SQL over the temporary table\n",
    "df_staging = spark.sql(\"\"\"  \n",
    "SELECT \n",
    "CS_D_ADAPPL_CNTR.INSTITUTION||'~'||CS_D_ADAPPL_CNTR.ADM_APPL_CTR AS UNIFICATION_ID,\n",
    "CS_D_ADAPPL_CNTR.INSTITUTION,\n",
    "CS_D_ADAPPL_CNTR.ADM_APPL_CTR,\n",
    "CS_D_ADAPPL_CNTR.EFFDT,\n",
    "CS_D_ADAPPL_CNTR.EFF_STATUS,\n",
    "CS_D_ADAPPL_CNTR.DESCR,\n",
    "CS_D_ADAPPL_CNTR.DESCRSHORT,\n",
    "CS_D_ADAPPL_CNTR.FEE_CODE,\n",
    "CS_D_ADAPPL_CNTR.ACAD_CAREER,\n",
    "CS_D_ADAPPL_CNTR.DEPOSIT_FEE_CD,\n",
    "CS_D_ADAPPL_CNTR.SF_MERCHANT_ID,\n",
    "CS_D_ADAPPL_CNTR.BATCH_APP_FEE_FLG,\n",
    "CS_D_ADAPPL_CNTR.BATCH_DEP_FEE_FLG,\n",
    "CS_D_ADAPPL_CNTR.SAD_CRM_SA_URL_ID,\n",
    "CS_D_ADAPPL_CNTR.SAD_CRM_CRM_URL_ID,\n",
    "CS_D_ADAPPL_CNTR.SAD_APP_FEE_REQ,\n",
    "CS_D_ADAPPL_CNTR.SAD_APP_FEE_WHEN,\n",
    "CS_D_ADAPPL_CNTR.SAD_APP_FEE_ALOWVR,\n",
    "CS_D_ADAPPL_CNTR.SAD_APP_FEE_MANPST,\n",
    "CS_D_ADAPPL_CNTR.SAD_APP_FEE_STATWV,\n",
    "CS_D_ADAPPL_CNTR.SF_MERCHANT_ID_EC,\n",
    "CS_D_ADAPPL_CNTR.TEL_CONTACT_1,\n",
    "CS_D_ADAPPL_CNTR.TEL_CONTACT_2\n",
    "FROM\n",
    "(\n",
    "SELECT\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.INSTITUTION)),'-') AS INSTITUTION,\n",
    "COALESCE(To_date(PS_ADM_APPLCTR_TBL.EFFDT,'mm-dd-yyyy')) AS EFFDT,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.ADM_APPL_CTR)),'-') AS ADM_APPL_CTR,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.EFF_STATUS)),'-') AS EFF_STATUS,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.DESCR)),'-') AS DESCR,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.DESCRSHORT)),'-') AS DESCRSHORT,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.FEE_CODE)),'-') AS FEE_CODE,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.ACAD_CAREER)),'-') AS ACAD_CAREER,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.DEPOSIT_FEE_CD)),'-') AS DEPOSIT_FEE_CD,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.SF_MERCHANT_ID)),'-') AS SF_MERCHANT_ID,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.BATCH_APP_FEE_FLG)),'-') AS BATCH_APP_FEE_FLG,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.BATCH_DEP_FEE_FLG)),'-') AS BATCH_DEP_FEE_FLG,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.SAD_CRM_SA_URL_ID)),'-') AS SAD_CRM_SA_URL_ID,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.SAD_CRM_CRM_URL_ID)),'-') AS SAD_CRM_CRM_URL_ID,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.SAD_APP_FEE_REQ)),'-') AS SAD_APP_FEE_REQ,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.SAD_APP_FEE_WHEN)),'-') AS SAD_APP_FEE_WHEN,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.SAD_APP_FEE_ALOWVR)),'-') AS SAD_APP_FEE_ALOWVR,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.SAD_APP_FEE_MANPST)),'-') AS SAD_APP_FEE_MANPST,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.SAD_APP_FEE_STATWV)),'-') AS SAD_APP_FEE_STATWV,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.SF_MERCHANT_ID_EC)),'-') AS SF_MERCHANT_ID_EC,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.TEL_CONTACT_1)),'-') AS TEL_CONTACT_1,\n",
    "COALESCE(RTRIM(LTRIM(PS_ADM_APPLCTR_TBL.TEL_CONTACT_2)),'-') AS TEL_CONTACT_2,\n",
    "DENSE_RANK()OVER(PARTITION BY PS_ADM_APPLCTR_TBL.INSTITUTION,PS_ADM_APPLCTR_TBL.ADM_APPL_CTR ORDER BY PS_ADM_APPLCTR_TBL.EFFDT DESC) AS DNSRNK\n",
    "FROM PS_ADM_APPLCTR_TBL PS_ADM_APPLCTR_TBL\n",
    ")CS_D_ADAPPL_CNTR\n",
    "WHERE CS_D_ADAPPL_CNTR.DNSRNK=1\n",
    " \"\"\")\n",
    "df_staging = df_staging.withColumn(\"EFFDT\", df_staging[\"EFFDT\"].cast(DateType()))\n",
    "# SEQ_ID need to remove\n",
    "#df_staging.printSchema()\n",
    "#df_staging.show()\n",
    "#df_staging=df_staging.withColumn(\"EFFDT\", to_date(unix_timestamp(\"EFFDT\", \"M/dd/yyyy\").cast(\"timestamp\")))\n",
    "#df_staging.printSchema()                      \n",
    "#df_staging.select(\"EFFDT\").show()                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_staging.write.option(\"header\", \"true\").mode('overwrite').csv(targetFileName)\n",
    "column_list = df_staging.schema.names\n",
    "def listToStr(lst):\n",
    "    columns = ', '.join([str(elem) for elem in lst])\n",
    "    return columns\n",
    "columns = listToStr(column_list)\n",
    "df_staging.registerTempTable('temp')\n",
    "df_staging = spark.sql(\"\"\"\n",
    "SELECT\n",
    "{},\n",
    "CURRENT_TIMESTAMP() INSERTED_DATE,\n",
    "CURRENT_TIMESTAMP() UPDATED_DATE,\n",
    "CAST('9999-01-01 00:00:00.000' AS TIMESTAMP) as DELETED_DATE\n",
    "from temp\n",
    "\"\"\".format(columns))\n",
    "\n",
    "#Below function generates the integers monotonically increasing and consecutive in a partition\n",
    "df_staging = df_staging.withColumn(SEQUENCE_ID,row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "# Needs to be moved to dim\n",
    "d=df_staging.dtypes\n",
    "#print(d)\n",
    "\n",
    "datatype=[]\n",
    "columnname=[]\n",
    "\n",
    "for x in d:\n",
    "    datatype.append(x[1])\n",
    "    columnname.append(x[0])\n",
    "\n",
    "#print(columnname)\n",
    "#print(datatype)\n",
    "\n",
    "n=['string', 'timestamp','int','date', 'long']\n",
    "\n",
    "for n, i in enumerate(datatype):\n",
    "    if i == 'string':\n",
    "        datatype[n] = '0'\n",
    "    elif i== 'int':\n",
    "        datatype[n]= 0\n",
    "    elif i=='timestamp':\n",
    "        datatype[n]='1900-01-01 01:01:01 UTC'\n",
    "    elif i=='date':\n",
    "        datatype[n]='1900-01-01'\n",
    "    elif i=='long':\n",
    "        datatype[n]= 0\n",
    "\n",
    "#TestSchema is the table name which in which data is loaded in BQ        \n",
    "#print(\"The data types of the table are\")\n",
    "#print(datatype)\n",
    "unspecifiedRow = sqlContext.createDataFrame([datatype],columnname)\n",
    "#unspecifiedRow.show()\n",
    "df_final = df_staging.union(unspecifiedRow)\n",
    "#df_final.write.option(\"header\", \"true\").mode('overwrite').csv(targetFileName)\n",
    "path = \"gs://sjsu_cdw/cdw_incremantal_load/TransformedData/Admissions/Dimensions/\"\n",
    "dfile = 'CS_D_ADAPPL_CNTR'\n",
    "df_final.write.option(\"header\", \"true\").mode('overwrite').csv(path+dfile)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}