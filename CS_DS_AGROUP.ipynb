{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://sjsu_cdw/config_files/\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import datetime\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "#Create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()\n",
    "CONFIG_PATH = os.environ.get('CONFIG_PATH')\n",
    "print(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://sjsu_cdw/raw_data/Admission/\n",
      "gs://sjsu_cdw/cdw_incremantal_load/TransformedData/Admissions/Dimension_Staging/\n"
     ]
    }
   ],
   "source": [
    "df_config = spark.read.option(\"header\", \"true\").csv(CONFIG_PATH+\"Config_ADM.csv\").collect()\n",
    "SOURCEFILEPATH = df_config[0]['SourceFile']\n",
    "TARGETFILEPATH = df_config[1]['adm_dim_path']\n",
    "fileName = \"CS_DS_AGROUP\"\n",
    "SEQUENCE_ID = df_config[0]['monotonically_inc_id']\n",
    "\n",
    "print(SOURCEFILEPATH)\n",
    "print(TARGETFILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all the required source tables\n",
    "sourceTablelist = [\"PS_ACAD_GROUP_TBL\"]\n",
    "#Get the target table filename from the python filename\n",
    "targetFileName =  TARGETFILEPATH+fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "#Iterate through the source table list and load the data into a Spark dataframe. \n",
    "def createTempTables(sparkSess,sourceTablelist):\n",
    "    for tabList in sourceTablelist:\n",
    "        df_source = sparkSess.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").option(\"nullValue\",\" \").csv(SOURCEFILEPATH+tabList+\".csv\")\n",
    "        #df1 = df_source.na.fill('-')\n",
    "        #df2=df1.na.fill('1900-01-01 01:01:01 UTC')\n",
    "        #df2.printSchema()\n",
    "        #df_source.printSchema()\n",
    "        #Register the dataframe as a temporary table\n",
    "        df_source.registerTempTable(tabList)\n",
    "    return df_source\n",
    "df_source=createTempTables(spark,sourceTablelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a Spark SQL over the temporary table\n",
    "df_staging = spark.sql(\"\"\"\n",
    "  \n",
    "SELECT CS_D_AGROUP.INSTITUTION||'~'||CS_D_AGROUP.ACAD_GROUP AS UNIFICATION_ID,\n",
    "CS_D_AGROUP.ACAD_GROUP,\n",
    "CS_D_AGROUP.EFFDT,\n",
    "CS_D_AGROUP.INSTITUTION,\n",
    "CS_D_AGROUP.EFF_STATUS,\n",
    "CS_D_AGROUP.DESCR,\n",
    "CS_D_AGROUP.DESCRSHORT,\n",
    "CS_D_AGROUP.STDNT_SPEC_PERM,\n",
    "CS_D_AGROUP.AUTO_ENRL_WAITLIST FROM \n",
    "\n",
    "(\n",
    "SELECT \n",
    "COALESCE(RTRIM(LTRIM(PS_ACAD_GROUP_TBL.INSTITUTION)),'-') AS INSTITUTION,\n",
    "COALESCE(RTRIM(LTRIM(PS_ACAD_GROUP_TBL.ACAD_GROUP)),'-') AS ACAD_GROUP,\n",
    "COALESCE(TO_DATE(PS_ACAD_GROUP_TBL.EFFDT,'dd/mm/yyyy'),'1900-01-01') AS EFFDT,\n",
    "COALESCE(RTRIM(LTRIM(PS_ACAD_GROUP_TBL.EFF_STATUS)),'-') AS EFF_STATUS,\n",
    "COALESCE(RTRIM(LTRIM(PS_ACAD_GROUP_TBL.DESCR)),'-') AS DESCR,\n",
    "COALESCE(RTRIM(LTRIM(PS_ACAD_GROUP_TBL.DESCRSHORT)),'-') AS DESCRSHORT,\n",
    "COALESCE(RTRIM(LTRIM(PS_ACAD_GROUP_TBL.STDNT_SPEC_PERM)),'-') AS STDNT_SPEC_PERM,\n",
    "COALESCE(RTRIM(LTRIM(PS_ACAD_GROUP_TBL.AUTO_ENRL_WAITLIST)),'-') AS AUTO_ENRL_WAITLIST,\n",
    "DENSE_RANK()OVER(PARTITION BY PS_ACAD_GROUP_TBL.INSTITUTION,PS_ACAD_GROUP_TBL.ACAD_GROUP  ORDER BY PS_ACAD_GROUP_TBL.EFFDT DESC) AS DNS_RNK\n",
    "FROM\n",
    "PS_ACAD_GROUP_TBL)  CS_D_AGROUP\n",
    "WHERE CS_D_AGROUP.DNS_RNK=1\n",
    "\n",
    " \"\"\")\n",
    "                       \n",
    "df_staging = df_staging.withColumn(\"EFFDT\", df_staging[\"EFFDT\"].cast(DateType()))\n",
    "                       \n",
    "#Below function generates the integers monotonically increasing and consecutive in a partition\n",
    "#df_staging = df_staging.withColumn(SEQUENCE_ID,row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "#df_staging.show()\n",
    "#df_staging=df_staging.withColumn(\"EFFDT\", to_date(unix_timestamp(\"EFFDT\", \"M/dd/yyyy\").cast(\"timestamp\")))\n",
    "#df_staging.printSchema()\n",
    "#df_staging.select(\"EFFDT\").show()                      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_staging.write.option(\"header\", \"true\").mode('overwrite').csv(targetFileName)\n",
    "column_list = df_staging.schema.names\n",
    "def listToStr(lst):\n",
    "    columns = ', '.join([str(elem) for elem in lst])\n",
    "    return columns\n",
    "columns = listToStr(column_list)\n",
    "df_staging.registerTempTable('temp')\n",
    "df_staging = spark.sql(\"\"\"\n",
    "SELECT\n",
    "{},\n",
    "CURRENT_TIMESTAMP() INSERTED_DATE,\n",
    "CURRENT_TIMESTAMP() UPDATED_DATE,\n",
    "CAST('9999-01-01 00:00:00.000' AS TIMESTAMP) as DELETED_DATE\n",
    "\n",
    "from temp\n",
    "\"\"\".format(columns))\n",
    "\n",
    "#Below function generates the integers monotonically increasing and consecutive in a partition\n",
    "df_staging = df_staging.withColumn(SEQUENCE_ID,row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "# Needs to be moved to dim\n",
    "d=df_staging.dtypes\n",
    "#print(d)\n",
    "\n",
    "datatype=[]\n",
    "columnname=[]\n",
    "\n",
    "for x in d:\n",
    "    datatype.append(x[1])\n",
    "    columnname.append(x[0])\n",
    "\n",
    "#print(columnname)\n",
    "#print(datatype)\n",
    "\n",
    "n=['string', 'timestamp','int','date', 'long']\n",
    "\n",
    "for n, i in enumerate(datatype):\n",
    "    if i == 'string':\n",
    "        datatype[n] = '0'\n",
    "    elif i== 'int':\n",
    "        datatype[n]= 0\n",
    "    elif i=='timestamp':\n",
    "        datatype[n]='1900-01-01 01:01:01 UTC'\n",
    "    elif i=='date':\n",
    "        datatype[n]='1900-01-01'\n",
    "    elif i=='long':\n",
    "        datatype[n]= 0\n",
    "\n",
    "#TestSchema is the table name which in which data is loaded in BQ        \n",
    "#print(\"The data types of the table are\")\n",
    "#print(datatype)\n",
    "unspecifiedRow = sqlContext.createDataFrame([datatype],columnname)\n",
    "#unspecifiedRow.show()\n",
    "df_final = df_staging.union(unspecifiedRow)\n",
    "#df_final.write.option(\"header\", \"true\").mode('overwrite').csv(targetFileName)\n",
    "path = \"gs://sjsu_cdw/cdw_incremantal_load/TransformedData/Admissions/Dimensions/\"\n",
    "dfile = 'CS_D_AGROUP'\n",
    "df_final.write.option(\"header\", \"true\").mode('overwrite').csv(path+dfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}