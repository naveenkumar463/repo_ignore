{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://sjsu_cdw/config_files/\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import row_number, monotonically_increasing_id\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import datetime\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "\n",
    "#Create a Spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .getOrCreate()\n",
    "CONFIG_PATH = os.environ.get('CONFIG_PATH')\n",
    "print(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://sjsu_cdw/raw_data/Admission/\n",
      "gs://sjsu_cdw/cdw_incremantal_load/TransformedData/Admissions/Dimension_Staging/\n"
     ]
    }
   ],
   "source": [
    "df_config = spark.read.option(\"header\", \"true\").csv(CONFIG_PATH+\"Config_ADM.csv\").collect()\n",
    "SOURCEFILEPATH = df_config[0]['SourceFile']\n",
    "TARGETFILEPATH = df_config[1]['adm_dim_path']\n",
    "fileName = \"CS_DS_ADMIT_TYPE\"\n",
    "SEQUENCE_ID = df_config[0]['monotonically_inc_id']\n",
    "\n",
    "print(SOURCEFILEPATH)\n",
    "print(TARGETFILEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List all the required source tables\n",
    "sourceTablelist = [\"PS_ADMIT_TYPE_TBL\"]\n",
    "#Get the target table filename from the python filename\n",
    "targetFileName =  TARGETFILEPATH+fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func\n",
    "#Iterate through the source table list and load the data into a Spark dataframe. \n",
    "def createTempTables(sparkSess,sourceTablelist):\n",
    "    for tabList in sourceTablelist:\n",
    "        df_source = sparkSess.read.option(\"header\", \"true\").option(\"inferSchema\",\"true\").option(\"nullValue\",\" \").csv(SOURCEFILEPATH+tabList+\".csv\")\n",
    "        #df1 = df_source.na.fill('-')\n",
    "        #df2=df1.na.fill('1900-01-01 01:01:01 UTC')\n",
    "        #df2.printSchema()\n",
    "        #df_source.printSchema()\n",
    "        #Register the dataframe as a temporary table\n",
    "        df_source.registerTempTable(tabList)\n",
    "    return df_source\n",
    "df_source=createTempTables(spark,sourceTablelist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run a Spark SQL over the temporary table\n",
    "df_staging = spark.sql(\"\"\"\n",
    " SELECT \n",
    "INSTITUTION||'~'||ADMIT_TYPE_CODE AS UNIFICATION_ID,\n",
    "INSTITUTION,\n",
    "ADMIT_TYPE_CODE,\n",
    "EFFDT,\n",
    "EFF_STATUS,\n",
    "ADMIT_TYPE_DESCR,\n",
    "ADMIT_TYPE,\n",
    "ACAD_CAREER,\n",
    "READMIT_PROCESS\n",
    "FROM(\n",
    "SELECT \n",
    "COALESCE(RTRIM(LTRIM(INSTITUTION)),'-') AS INSTITUTION,\n",
    "COALESCE(RTRIM(LTRIM(ADMIT_TYPE)),'-') AS ADMIT_TYPE_CODE,\n",
    "COALESCE(TO_DATE(EFFDT,'dd/mm/yyyy'),'1900-01-01') AS EFFDT,\n",
    "COALESCE(RTRIM(LTRIM(EFF_STATUS)),'-') AS EFF_STATUS,\n",
    "COALESCE(RTRIM(LTRIM(DESCR)),'-') AS ADMIT_TYPE_DESCR,\n",
    "COALESCE(RTRIM(LTRIM(DESCRSHORT)),'-') AS ADMIT_TYPE,\n",
    "COALESCE(RTRIM(LTRIM(ACAD_CAREER)),'-') AS ACAD_CAREER,\n",
    "COALESCE(RTRIM(LTRIM(READMIT_PROCESS)),'-') AS READMIT_PROCESS,\n",
    "DENSE_RANK()OVER(PARTITION BY INSTITUTION,ADMIT_TYPE ORDER BY EFFDT DESC )DNS_RANK\n",
    " FROM PS_ADMIT_TYPE_TBL ) CS_D_ADMIT_TYPE\n",
    " WHERE\n",
    " DNS_RANK=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \"\"\")\n",
    "#Below function generates the integers monotonically increasing and consecutive in a partition\n",
    "#df_staging.printSchema()\n",
    "#df_staging.show()\n",
    "#df_staging=df_staging.withColumn(\"EFFDT\", to_date(unix_timestamp(\"EFFDT\", \"M/dd/yyyy\").cast(\"timestamp\")))\n",
    "df_staging = df_staging.withColumn(\"EFFDT\", df_staging[\"EFFDT\"].cast(DateType()))\n",
    "#df_staging.printSchema()\n",
    "#df_staging.select(\"EFFDT\").show()                      \n",
    "                                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_staging.write.option(\"header\", \"true\").mode('overwrite').csv(targetFileName)\n",
    "column_list = df_staging.schema.names\n",
    "def listToStr(lst):\n",
    "    columns = ', '.join([str(elem) for elem in lst])\n",
    "    return columns\n",
    "columns = listToStr(column_list)\n",
    "df_staging.registerTempTable('temp')\n",
    "df_staging = spark.sql(\"\"\"\n",
    "SELECT\n",
    "{},\n",
    "CURRENT_TIMESTAMP() INSERTED_DATE,\n",
    "CURRENT_TIMESTAMP() UPDATED_DATE,\n",
    "CAST('9999-01-01 00:00:00.000' AS TIMESTAMP) as DELETED_DATE\n",
    "\n",
    "from temp\n",
    "\"\"\".format(columns))\n",
    "\n",
    "#Below function generates the integers monotonically increasing and consecutive in a partition\n",
    "df_staging = df_staging.withColumn(SEQUENCE_ID,row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "# Needs to be moved to dim\n",
    "d=df_staging.dtypes\n",
    "#print(d)\n",
    "\n",
    "datatype=[]\n",
    "columnname=[]\n",
    "\n",
    "for x in d:\n",
    "    datatype.append(x[1])\n",
    "    columnname.append(x[0])\n",
    "\n",
    "#print(columnname)\n",
    "#print(datatype)\n",
    "\n",
    "n=['string', 'timestamp','int','date', 'long']\n",
    "\n",
    "for n, i in enumerate(datatype):\n",
    "    if i == 'string':\n",
    "        datatype[n] = '0'\n",
    "    elif i== 'int':\n",
    "        datatype[n]= 0\n",
    "    elif i=='timestamp':\n",
    "        datatype[n]='1900-01-01 01:01:01 UTC'\n",
    "    elif i=='date':\n",
    "        datatype[n]='1900-01-01'\n",
    "    elif i=='long':\n",
    "        datatype[n]= 0\n",
    "\n",
    "#TestSchema is the table name which in which data is loaded in BQ        \n",
    "#print(\"The data types of the table are\")\n",
    "#print(datatype)\n",
    "unspecifiedRow = sqlContext.createDataFrame([datatype],columnname)\n",
    "#unspecifiedRow.show()\n",
    "df_final = df_staging.union(unspecifiedRow)\n",
    "#df_final.write.option(\"header\", \"true\").mode('overwrite').csv(targetFileName)\n",
    "path = \"gs://sjsu_cdw/cdw_incremantal_load/TransformedData/Admissions/Dimensions/\"\n",
    "dfile = 'CS_D_ADMIT_TYPE'\n",
    "df_final.write.option(\"header\", \"true\").mode('overwrite').csv(path+dfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}